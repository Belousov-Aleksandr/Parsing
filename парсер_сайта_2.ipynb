{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:96.0) Gecko/20100101 Firefox/96.0'}\n",
    "FQDN = 'https://www.y*******.**/'\n",
    "responce = requests.get(FQDN)\n",
    "\n",
    "print(\"request denied\") if responce.status_code != 200 else print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb80ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url, params=None):\n",
    "    r = requests.get(url, headers=header, params=params)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67866512",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.y*******.**/search/'\n",
    "response = get_html(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432981a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "title=[]\n",
    "types_all=[]\n",
    "deadline=[]\n",
    "url=[]\n",
    "description=[]\n",
    "\n",
    "last_page = max([int(e.text) for e in soup.find_all('a', class_='page-numbers') if e.text != 'Â»'])\n",
    "page_links = [url + '?sf_paged=' + str(i+1) if i > 0 else url for i in range(last_page)]\n",
    "\n",
    "for page in page_links[:]:\n",
    "    response = get_html(page)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    links = [e.a.get('href') for e in soup.find_all('div', class_='post-header')]\n",
    "    for link in links[:]:\n",
    "        response = get_html(link)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        types = [e.text for e in soup.find_all('a', class_='meta-category')][0]\n",
    "        types_all.append(types.replace('\\n', ''))\n",
    "        url.append(link)\n",
    "        ddl = [e.li.text if e.li is not None else '' for e in soup.find_all('ul', class_='post-details')][0]\n",
    "        deadline.append(ddl.replace('Deadline ', '')) if 'Deadline' in ddl else deadline.append('')\n",
    "        p_tags=[e.find_all('p') for e in soup.find_all('div', class_='article-content')][0]\n",
    "        description.append([p.text for p in p_tags[:5]])\n",
    "        title.append([e for e in soup.find('h1', class_='page-title')][0])\n",
    "    time.sleep(1)   \n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(page)\n",
    "    print(len(title),len(deadline),len(description),len(url), len(types_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a5b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['course_name'] = title\n",
    "df['course_type'] = types_all\n",
    "df['url'] = url\n",
    "df['deadline'] = deadline\n",
    "df['description'] = description\n",
    "\n",
    "descr = []\n",
    "for idx,_ in df.iterrows():\n",
    "    descr.append(', '.join(map(str, df.iloc[idx]['description'])))\n",
    "df['description'] = descr\n",
    "\n",
    "with pd.ExcelWriter('y******.xlsx') as writer:\n",
    "    df.to_excel(writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
