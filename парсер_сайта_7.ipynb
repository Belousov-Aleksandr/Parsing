{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime, date\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:96.0) Gecko/20100101 Firefox/96.0'}\n",
    "FQDN = 'https://www.m******.**/'\n",
    "responce = requests.get(FQDN)\n",
    "\n",
    "print(\"request denied\") if responce.status_code != 200 else print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb80ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url, params=None):\n",
    "    r = requests.get(url, headers=header, params=params)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fffcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(L: list) -> list:\n",
    "    '''рекурентное преобразование списка списков списков... в плоский одноранговый список'''\n",
    "    if len(L) > sys.getrecursionlimit():\n",
    "        sys.setrecursionlimit(len(L)*10)\n",
    "    if L == []:\n",
    "        return L\n",
    "    if isinstance(L[0], list):\n",
    "        return flatten(L[0]) + flatten(L[1:])\n",
    "    rez = list(L[:1] + flatten(L[1:]))\n",
    "    return rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_date(str_date: str) -> datetime.date:\n",
    "    '''принимает год/день/месяц ('22/20/04') , возвращает в формате datetime'''\n",
    "    temp_date = [d for d in str_date.split('/')]\n",
    "    temp_date.reverse()\n",
    "    temp_date = '-'.join(temp_date)\n",
    "    return date.fromisoformat(temp_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ba7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_urls_on_page(page_url: str) -> list:\n",
    "    '''возвращает список дат и ссылок на статьи со страницы'''\n",
    "    response = get_html(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    dates = [e.text for e in soup.find_all('div', class_='news-list-item-date')]\n",
    "    short_urls = [e.a.get('href') for e in soup.find_all('div', class_='news-list-item-head')]\n",
    "    fqdn_urls = ['/'.join(base_url.split('/')[:3]) + elem for elem in short_urls]\n",
    "    \n",
    "    return dates, fqdn_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35f87d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(page_url: str):\n",
    "    '''сборщик контента. возвращает 3 списка - текст, внутренние ссылки, ссылки на картинки'''\n",
    "    response = get_html(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    text_content = []\n",
    "    urls_content = []\n",
    "    img_content = []\n",
    "\n",
    "    for e in soup.find_all('div', class_='news-list-item-text'):\n",
    "            \n",
    "        text_content.append(e.text.strip().replace('\\n', '').replace('\\r', '').replace('\\t', '').replace('\\xa0', ' '))\n",
    "\n",
    "        for i in e.find_all('img'):\n",
    "            img_content.append('/'.join(base_url.split('/')[:3]) + i.prettify().split('src=')[1].split('\"')[1])\n",
    "\n",
    "        for i in e.find_all('a'):\n",
    "            if i.get('href').startswith('/'):\n",
    "                urls_content.append('/'.join(base_url.split('/')[:3]) + str(i.get('href')))\n",
    "            elif i.get('href').startswith('http'):\n",
    "                urls_content.append(i.get('href'))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    return text_content, urls_content, img_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1398670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция не задействуется, потому что структура сайта другая\n",
    "def id_urls_on_page(page_url: str) -> list:\n",
    "    '''возвращает список id и ссылок на Объявления со страницы'''\n",
    "    response = get_html(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    for e in soup.find_all('div', class_='catalog-section'):\n",
    "        e.find('div', class_='pagination').extract()\n",
    "        short_urls = [elem.get('href') for elem in e.find_all('a')]\n",
    "    fqdn_urls = ['/'.join(base_url.split('/')[:3]) + elem for elem in short_urls[::2]]\n",
    "    id_list  = [int(i.split('/')[3]) for i in short_urls[::2]]\n",
    "    \n",
    "    return id_list, fqdn_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_column(df: pd.core.frame.DataFrame, column: str) -> pd.core.frame.DataFrame:\n",
    "    '''извлекает значения в ячейках датафрейма из списочного вида в текстовый'''\n",
    "\n",
    "    for idx in range(df.shape[0]):\n",
    "        if type(df.loc[idx,column]) is list:\n",
    "            df.loc[idx,column] = ', '.join(df.loc[idx,column]) if len(df.loc[idx,column]) > 0 else ''\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeba69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conf(fqdn: str, start_id: int = 0, start_date: str = '2022-01-01'):\n",
    "    file_name = 'aggregator.ini'\n",
    "    domain = '.'.join(fqdn.split('/')[2].split('.')[-2:])\n",
    "    start_date = date.fromisoformat(start_date)\n",
    "    if not os.path.exists(file_name):\n",
    "        open(file_name, 'x')\n",
    "    with open(file_name) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            if domain in row[0]:\n",
    "                try:\n",
    "                    start_date = date.fromisoformat(row[1])\n",
    "                except (ValueError):\n",
    "                    start_date = start_date\n",
    "                try:\n",
    "                    start_id = abs(int(float(row[2])))\n",
    "                except (ValueError):\n",
    "                    start_id = start_id\n",
    "                    \n",
    "    return start_date, start_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_conf(fqdn: str, end_id: int = 0, end_date: str = str(datetime.now().date())):\n",
    "    domain = '.'.join(fqdn.split('/')[2].split('.')[-2:])\n",
    "    file_name = 'aggregator.ini'\n",
    "    fin = open(file_name, \"rt\")\n",
    "    data = fin.read()\n",
    "    domain_list = []\n",
    "    for elem in data.split('\\n'):\n",
    "        domain_list.append(elem.split(',')[0])\n",
    "        if elem.split(',')[0] == domain:\n",
    "            data = data.replace(domain + ',' + elem.split(',')[1] +',' + elem.split(',')[2], \n",
    "                                domain + ',' + end_date + ',' + str(end_id))\n",
    "            break\n",
    "    \n",
    "    new_row = domain + ',' + end_date + ',' + str(end_id)\n",
    "    if domain not in domain_list:\n",
    "        data += new_row if len(domain_list[0]) == 0 else '\\n' + new_row\n",
    "\n",
    "    fin.close()\n",
    "    fin = open(file_name, \"wt\")\n",
    "    fin.write(data)\n",
    "    fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba13cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date,_  = read_conf(FQDN)\n",
    "print(start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e74835",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6cbbdd",
   "metadata": {},
   "source": [
    "###### Раздел \"Новости\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.m*****.**/news/?PAGEN_1='\n",
    "start_page = base_url + str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates=[]\n",
    "urls=[]\n",
    "_, urls_on_page = dates_urls_on_page(start_page) # все новостные ссылки с первой разводящей страницы\n",
    "urls.extend(urls_on_page)\n",
    "for page_url in urls_on_page:\n",
    "    response = get_html(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    dates.extend([e.text.split('/')[0] + '/' + e.text.split('/')[1] + '/' + '20' + e.text.split('/')[2] \\\n",
    "                      if len(e.text.split('/')[2]) < 4 else e.text \\\n",
    "                      for e in soup.find_all('div', class_='news-list-item-date')])\n",
    "\n",
    "count=1\n",
    "while str_to_date(dates[-1]) > start_date:\n",
    "    count += 1\n",
    "    next_page = base_url + str(count)\n",
    "    _, urls_on_page = dates_urls_on_page(next_page)\n",
    "    urls.extend(urls_on_page)\n",
    "    for page_url in urls_on_page:\n",
    "        response = get_html(page_url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        dates.extend([e.text.split('/')[0] + '/' + e.text.split('/')[1] + '/' + '20' + e.text.split('/')[2] \\\n",
    "                          if len(e.text.split('/')[2]) < 4 else e.text \\\n",
    "                          for e in soup.find_all('div', class_='news-list-item-date')])\n",
    "        \n",
    "# Количество новых статей с даты последнего запуска агрегатора\n",
    "news_sum = sum([True for date_news in dates if str_to_date(date_news) > start_date])\n",
    "\n",
    "print(f'С даты последнего запуска агрегатора [{start_date}] опубликовано {news_sum} статей.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a8e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "title=[]\n",
    "content = []\n",
    "ext_urls = []\n",
    "images = []\n",
    "for page_url in urls[:news_sum]:\n",
    "    response = get_html(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    title.append([e for e in soup.find('h1')][0])\n",
    "    content.append(get_content(page_url)[0])\n",
    "    ext_urls.append(get_content(page_url)[1])\n",
    "    images.append(get_content(page_url)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88eb4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame()\n",
    "df1['Дата'] = dates[:news_sum]\n",
    "df1['Раздел'] = 'Новости ВУЗА'\n",
    "df1['Заголовок'] = title\n",
    "df1['Текст'] = content\n",
    "df1['Внешние ссылки'] = ext_urls\n",
    "#df1['Графика'] = images\n",
    "df1['URL статьи'] = urls[:news_sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a31fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# корректируем столбцы 'Текст' и 'Внешние ссылки'\n",
    "for column in list(df1)[3:5]:\n",
    "    df1 = flatten_column(df1,column)\n",
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc53e2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c7f5c",
   "metadata": {},
   "source": [
    "###### Раздел \"Объявления\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a35dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.m*******.**/ad/all/?PAGEN_1='\n",
    "start_page = base_url + str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f303d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates=[]\n",
    "urls=[]\n",
    "_, urls_on_page = dates_urls_on_page(start_page) # все объявления с первой разводящей страницы\n",
    "urls.extend(urls_on_page)\n",
    "for page_url in urls_on_page:\n",
    "    response = get_html(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    dates.extend([e.text.split('/')[0] + '/' + e.text.split('/')[1] + '/' + '20' + e.text.split('/')[2] \\\n",
    "                      if len(e.text.split('/')[2]) < 4 else e.text \\\n",
    "                      for e in soup.find_all('div', class_='news-list-item-date')])\n",
    "\n",
    "count=1\n",
    "while str_to_date(dates[-1]) > start_date:\n",
    "    count += 1\n",
    "    next_page = base_url + str(count)\n",
    "    _, urls_on_page = dates_urls_on_page(next_page)\n",
    "    urls.extend(urls_on_page)\n",
    "    for page_url in urls_on_page:\n",
    "        response = get_html(page_url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        dates.extend([e.text.split('/')[0] + '/' + e.text.split('/')[1] + '/' + '20' + e.text.split('/')[2] \\\n",
    "                          if len(e.text.split('/')[2]) < 4 else e.text \\\n",
    "                          for e in soup.find_all('div', class_='news-list-item-date')])\n",
    "        \n",
    "# Количество новых объявлений с даты последнего запуска агрегатора\n",
    "news_sum = sum([True for date_news in dates if str_to_date(date_news) > start_date])\n",
    "\n",
    "print(f'С даты последнего запуска агрегатора [{start_date}] опубликовано {news_sum} объявлений.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b883cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "title=[]\n",
    "content = []\n",
    "ext_urls = []\n",
    "images = []\n",
    "for page_url in urls[:news_sum]:\n",
    "    response = get_html(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    title.append([e for e in soup.find('h1')][0])\n",
    "    content.append(get_content(page_url)[0])\n",
    "    ext_urls.append(get_content(page_url)[1])\n",
    "    images.append(get_content(page_url)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29847727",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "df2['Дата'] = dates[:news_sum]\n",
    "df2['Раздел'] = 'Объявления'\n",
    "df2['Заголовок'] = title\n",
    "df2['Текст'] = content\n",
    "df2['Внешние ссылки'] = ext_urls\n",
    "#df2['Графика'] = images\n",
    "df2['URL статьи'] = urls[:news_sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52717b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# корректируем столбцы 'Текст' и 'Внешние ссылки'\n",
    "for column in list(df2)[3:5]:\n",
    "    df2 = flatten_column(df2,column)\n",
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae036f18",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = base_url.split('/')[2].split('.')[1] + '_' + str(datetime.now().date()) + '.xlsx'\n",
    "with pd.ExcelWriter(file_name) as writer:\n",
    "    df1.to_excel(writer, sheet_name='Новости ВУЗа', index=False)\n",
    "    df2.to_excel(writer, sheet_name='Объявления', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a63d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#обновление даты в файле aggregator.ini\n",
    "write_conf(FQDN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52301e49",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
