{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime, date, timedelta\n",
    "import os\n",
    "import re\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:96.0) Gecko/20100101 Firefox/96.0'}\n",
    "FQDN = 'https://www.h******.**'\n",
    "responce = requests.get(FQDN)\n",
    "\n",
    "print(\"request denied\") if responce.status_code != 200 else print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb80ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url, params=None):\n",
    "    r = requests.get(url, headers=header, params=params)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fffcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(L: list) -> list:\n",
    "    '''рекурентное преобразование списка списков списков... в плоский одноранговый список'''\n",
    "    if len(L) > sys.getrecursionlimit():\n",
    "        sys.setrecursionlimit(len(L)*10)\n",
    "    if L == []:\n",
    "        return L\n",
    "    if isinstance(L[0], list):\n",
    "        return flatten(L[0]) + flatten(L[1:])\n",
    "    rez = list(L[:1] + flatten(L[1:]))\n",
    "    return rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_date(str_date: str) -> datetime.date:\n",
    "    '''принимает дату 12.11.2017 в формате string, возвращает в формате datetime'''\n",
    "    temp_date = [d for d in str_date.split('.')]\n",
    "    temp_date.reverse()\n",
    "    temp_date = '-'.join(temp_date)\n",
    "    return date.fromisoformat(temp_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ba7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_urls_titles_on_page(page_url: str) -> list:\n",
    "    '''возвращает список дат, ссылок, тайтлов на статьи со страницы'''\n",
    "    response = get_html(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    short_urls = [e.a.get('href') for e in soup.find_all('h2', class_='first_child')][::-1]\n",
    "    fqdn_urls = [FQDN + elem if 'http' not in elem and elem.startswith('/') else elem for elem in short_urls]\n",
    "    cur_date = '.'.join(re.findall('(\\d+)', page_url)[::-1])\n",
    "    dates = [cur_date] * len(short_urls)\n",
    "    titles = [e.a.text for e in soup.find_all('h2', class_='first_child')][::-1]\n",
    "    return dates, fqdn_urls, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d22ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_urls_titles_on_page_filial(page_url: str) -> list:\n",
    "    \n",
    "    '''возвращает список дат, ссылок, тайтлов на статьи со страницы'''\n",
    "    dates = []\n",
    "    fqdn_urls = []\n",
    "    month_list = ['янв', 'фев', 'мар', 'апр', 'мая', 'июн',\n",
    "           'июл', 'авг', 'сен', 'окт', 'ноя', 'дек']\n",
    "    response = get_html(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    short_urls = [e.a.get('href') for e in soup.find_all('h2', class_='first_child')]\n",
    "    for elem in short_urls:\n",
    "        if 'http' not in elem and elem.startswith('/') and 'h******.**' not in elem:\n",
    "            fqdn_urls.append(FQDN + elem)\n",
    "        elif 'http' not in elem and elem.startswith('/'):\n",
    "            fqdn_urls.append('https:' + elem)\n",
    "        else:\n",
    "            fqdn_urls.append(elem)\n",
    "    \n",
    "    for e in soup.find_all('div', class_='post-meta__date'):\n",
    "        day = e.get_text(' ', strip=True).split()[0]\n",
    "        month = str(month_list.index(e.get_text(' ', strip=True).split()[1]) + 1)\n",
    "        year = e.get_text(' ', strip=True).split()[2]\n",
    "        if len(month) < 2:\n",
    "            month = '0' + month\n",
    "        if len(day) < 2:\n",
    "            day = '0' + day\n",
    "        dates.append(day + '.' + month + '.' + year)\n",
    "    \n",
    "    titles = [e.a.text for e in soup.find_all('h2', class_='first_child')]\n",
    "    return dates, fqdn_urls, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35f87d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(page_url: str):\n",
    "    '''сборщик контента. возвращает 2 списка - текст, внутренние ссылки'''\n",
    "    response = get_html(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    text_content = [e.get_text(' ', strip=True).replace('\\xa0',' ').replace('\\t','').replace('\\n',' ').replace('  ',' ') \\\n",
    "                    for e in soup.find_all('div', class_='post__text')]\n",
    "    \n",
    "    if len(text_content) == 0:\n",
    "        for e in soup.find_all('div', class_='events__content post post_first'):\n",
    "            try:\n",
    "                e.find('div', class_='flex flex--gap').extract()\n",
    "            except (AttributeError):\n",
    "                pass\n",
    "            try:\n",
    "                e.find('h5').extract()\n",
    "            except (AttributeError):\n",
    "                pass\n",
    "            try:\n",
    "                e.find('div', class_='signup with-indent6').extract()\n",
    "            except (AttributeError):\n",
    "                pass\n",
    "            try:\n",
    "                e.find('form').extract()\n",
    "            except (AttributeError):\n",
    "                pass\n",
    "            \n",
    "            text_content.append(e.get_text(' ', strip=True).replace('\\xa0',' ').replace('\\t','').replace('\\n',' ').replace('  ',' '))\n",
    "      \n",
    "    if len(text_content) == 0:\n",
    "        text_content = [e.get_text(' ', strip=True).replace('\\xa0',' ').replace('\\t','').replace('\\n',' ').replace('  ',' ') \\\n",
    "                for e in soup.find_all('div', class_='b-row')]\n",
    "\n",
    "    if len(text_content) == 0:\n",
    "        text_content = [e.get_text(' ', strip=True).replace('\\xa0',' ').replace('\\t','').replace('\\n',' ').replace('  ',' ') \\\n",
    "                for e in soup.find_all('div', class_='block-news-single__content')]\n",
    "\n",
    "    if len(text_content) == 0:\n",
    "        text_content = [e.get_text(' ', strip=True).replace('\\xa0',' ').replace('\\t','').replace('\\n',' ').replace('  ',' ') \\\n",
    "                for e in soup.find_all('div', class_='container-fluid')]\n",
    "        \n",
    "    \n",
    "    urls_content = [[FQDN + i.get('href') if 'http' not in i.get('href') and 'h*****.**' not in i.get('href') \\\n",
    "                     else i.get('href') for i in e.find_all('a', href=True)] \\\n",
    "                    for e in soup.find_all('div', class_='post__text')]\n",
    "    \n",
    "    if len(urls_content) == 0:\n",
    "        for e in soup.find_all('div', class_='events__content post post_first'):\n",
    "            try:\n",
    "                e.find('div', class_='flex flex--gap').extract()\n",
    "            except (AttributeError):\n",
    "                pass\n",
    "            try:\n",
    "                e.find('h5').extract()\n",
    "            except (AttributeError):\n",
    "                pass\n",
    "            try:\n",
    "                e.find('div', class_='signup with-indent6').extract()\n",
    "            except (AttributeError):\n",
    "                pass\n",
    "            try:\n",
    "                e.find('form').extract()\n",
    "            except (AttributeError):\n",
    "                pass\n",
    "            \n",
    "            urls_content = [FQDN + i.get('href') if 'http' not in i.get('href') and 'h******.**' not in i.get('href') \\\n",
    "                            else i.get('href') for i in e.find_all('a', href=True)]\n",
    "            \n",
    "    if len(urls_content) == 0:        \n",
    "        urls_content = [[FQDN + i.get('href') if 'http' not in i.get('href') and 'h******.**' not in i.get('href') \\\n",
    "                     else i.get('href') for i in e.find_all('a', href=True)] \\\n",
    "                    for e in soup.find_all('div', class_='b-row')]\n",
    "        \n",
    "    if len(urls_content) == 0:        \n",
    "        urls_content = [[FQDN + i.get('href') if 'http' not in i.get('href') and 'h*******.**' not in i.get('href') \\\n",
    "                     else i.get('href') for i in e.find_all('a', href=True)] \\\n",
    "                    for e in soup.find_all('div', class_='block-news-single__content')]        \n",
    "\n",
    "    if len(urls_content) == 0:        \n",
    "        urls_content = [[FQDN + i.get('href') if 'http' not in i.get('href') and 'h******.**' not in i.get('href') \\\n",
    "                     else i.get('href') for i in e.find_all('a', href=True)] \\\n",
    "                    for e in soup.find_all('div', class_='container-fluid')]   \n",
    "    \n",
    "    \n",
    "    if len(urls_content) != 0 and isinstance(urls_content[0], list):\n",
    "        return text_content, [url for url in urls_content[0] if 'jpg' not in url.lower()]\n",
    "    else:\n",
    "        return text_content, [url for url in urls_content if 'jpg' not in url.lower()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c639e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_column(df: pd.core.frame.DataFrame, column: str) -> pd.core.frame.DataFrame:\n",
    "    '''извлекает значения в ячейках датафрейма из списочного вида в текстовый'''\n",
    "\n",
    "    for idx in range(df.shape[0]):\n",
    "\n",
    "        if type(df.loc[idx,column]) is list:\n",
    "            df.loc[idx,column] = ', '.join(df.loc[idx,column]) if len(df.loc[idx,column]) > 0 else ''\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeba69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conf(fqdn: str, start_id: int = 0, start_date: str = '2022-01-01'):\n",
    "    file_name = 'aggregator.ini'\n",
    "    domain = '.'.join(fqdn.split('/')[2].split('.')[-2:])\n",
    "    start_date = date.fromisoformat(start_date)\n",
    "    if not os.path.exists(file_name):\n",
    "        open(file_name, 'x')\n",
    "    with open(file_name) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            if domain in row[0]:\n",
    "                try:\n",
    "                    start_date = date.fromisoformat(row[1])\n",
    "                except (ValueError):\n",
    "                    start_date = start_date\n",
    "                try:\n",
    "                    start_id = abs(int(float(row[2])))\n",
    "                except (ValueError):\n",
    "                    start_id = start_id\n",
    "                    \n",
    "    return start_date, start_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_conf(fqdn: str, end_id: int, end_date: str = str(datetime.now().date())):\n",
    "    domain = '.'.join(fqdn.split('/')[2].split('.')[-2:])\n",
    "    file_name = 'aggregator.ini'\n",
    "    fin = open(file_name, \"rt\")\n",
    "    data = fin.read()\n",
    "    domain_list = []\n",
    "    for elem in data.split('\\n'):\n",
    "        domain_list.append(elem.split(',')[0])\n",
    "        if elem.split(',')[0] == domain:\n",
    "            data = data.replace(domain + ',' + elem.split(',')[1] +',' + elem.split(',')[2], \n",
    "                                domain + ',' + end_date + ',' + str(end_id))\n",
    "            break\n",
    "    \n",
    "    new_row = domain + ',' + end_date + ',' + str(end_id)\n",
    "    if domain not in domain_list:\n",
    "        data += new_row if len(domain_list[0]) == 0 else '\\n' + new_row\n",
    "\n",
    "    fin.close()\n",
    "    fin = open(file_name, \"wt\")\n",
    "    fin.write(data)\n",
    "    fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba13cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date, start_id  = read_conf(FQDN, start_id = 0, start_date = '2022-01-01') #id объявления\n",
    "print(f'start_date = {start_date}\\nstart_id = {start_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e74835",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6cbbdd",
   "metadata": {},
   "source": [
    "###### Раздел \"Город\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.h*****.**/news/archive/'\n",
    "start_page = base_url + '/'.join(str(start_date).split('-'))\n",
    "print(f'start_page = {start_page}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9181faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "dates = []\n",
    "urls = []\n",
    "titles = []\n",
    "current_page = start_page\n",
    "\n",
    "while datetime.now().date() >= start_date + timedelta(days = count):\n",
    "    responce = requests.get(current_page)\n",
    "    count += 1\n",
    "    if responce.status_code == 200:\n",
    "        dates.extend(dates_urls_titles_on_page(current_page)[0])\n",
    "        urls.extend(dates_urls_titles_on_page(current_page)[1])\n",
    "        titles.extend(dates_urls_titles_on_page(current_page)[2])\n",
    "        current_page = base_url + '/'.join(str(start_date + timedelta(days = count)).split('-'))\n",
    "    else:\n",
    "        current_page = base_url + '/'.join(str(start_date + timedelta(days = count)).split('-'))\n",
    "titles = titles[::-1]\n",
    "dates = dates[::-1]\n",
    "urls = urls[::-1]\n",
    "\n",
    "# Количество новых статей с даты последнего запуска агрегатора\n",
    "news_sum = sum([True for date_news in dates if str_to_date(date_news) > start_date])\n",
    "\n",
    "print(f'С даты последнего запуска агрегатора [{start_date}] опубликовано {news_sum} статей.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7346ff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content = []\n",
    "ext_urls = []\n",
    "\n",
    "for page_url in urls[:news_sum]:\n",
    "    print(page_url)\n",
    "    response = get_html(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    content.append(get_content(page_url)[0])\n",
    "    ext_urls.append(get_content(page_url)[1])\n",
    "    clear_output(wait=True)\n",
    "    print(len(dates[:news_sum]),len(urls[:news_sum]),len(titles[:news_sum]),len(content),len(ext_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88eb4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['Дата'] = dates[:news_sum]\n",
    "df['Раздел'] = '*******'\n",
    "df['Заголовок'] = titles[:news_sum]\n",
    "df['Текст'] = content\n",
    "df['Внешние ссылки'] = ext_urls\n",
    "df['URL статьи'] = urls[:news_sum]\n",
    "\n",
    "# корректируем столбцы 'Текст' и 'Внешние ссылки'\n",
    "for column in list(df)[3:5]:\n",
    "    df = flatten_column(df,column)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c76906",
   "metadata": {},
   "source": [
    "###### Раздел \"Город2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://****.h*******.**/news/page'\n",
    "start_page = base_url + str(1) + '.html'\n",
    "print(f'start_page = {start_page}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8ae6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "urls = []\n",
    "titles = []\n",
    "dates.extend(dates_urls_titles_on_page_filial(start_page)[0])\n",
    "urls.extend(dates_urls_titles_on_page_filial(start_page)[1])\n",
    "titles.extend(dates_urls_titles_on_page_filial(start_page)[2])\n",
    "\n",
    "count=1\n",
    "while str_to_date(dates[-1]) > start_date:\n",
    "    count += 1\n",
    "    next_page = base_url + str(count) + '.html'\n",
    "    dates.extend(dates_urls_titles_on_page_filial(next_page)[0])\n",
    "    urls.extend(dates_urls_titles_on_page_filial(next_page)[1])\n",
    "    titles.extend(dates_urls_titles_on_page_filial(next_page)[2])\n",
    "\n",
    "# Количество новых статей с даты последнего запуска агрегатора\n",
    "news_sum = sum([True for date_news in dates if str_to_date(date_news) > start_date])\n",
    "\n",
    "print(f'С даты последнего запуска агрегатора [{start_date}] опубликовано {news_sum} статей.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "ext_urls = []\n",
    "\n",
    "for page_url in urls[:news_sum]:\n",
    "    print(page_url)\n",
    "    response = get_html(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    content.append(get_content(page_url)[0])\n",
    "    ext_urls.append(get_content(page_url)[1])\n",
    "    clear_output(wait=True)\n",
    "    print(len(dates[:news_sum]),len(urls[:news_sum]),len(titles[:news_sum]),len(content),len(ext_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218fa335",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "df2['Дата'] = dates[:news_sum]\n",
    "df2['Раздел'] = '********'\n",
    "df2['Заголовок'] = titles[:news_sum]\n",
    "df2['Текст'] = content\n",
    "df2['Внешние ссылки'] = ext_urls\n",
    "df2['URL статьи'] = urls[:news_sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2625f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# корректируем столбцы 'Текст' и 'Внешние ссылки'\n",
    "for column in list(df2)[3:5]:\n",
    "    df2 = flatten_column(df2,column)\n",
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203f1dc0",
   "metadata": {},
   "source": [
    "###### Раздел \"Город3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd2968",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://***.h******.**/news/page'\n",
    "start_page = base_url + str(1) + '.html'\n",
    "print(f'start_page = {start_page}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4048e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "urls = []\n",
    "titles = []\n",
    "dates.extend(dates_urls_titles_on_page_filial(start_page)[0])\n",
    "urls.extend(dates_urls_titles_on_page_filial(start_page)[1])\n",
    "titles.extend(dates_urls_titles_on_page_filial(start_page)[2])\n",
    "\n",
    "count=1\n",
    "while str_to_date(dates[-1]) > start_date:\n",
    "    count += 1\n",
    "    next_page = base_url + str(count) + '.html'\n",
    "    dates.extend(dates_urls_titles_on_page_filial(next_page)[0])\n",
    "    urls.extend(dates_urls_titles_on_page_filial(next_page)[1])\n",
    "    titles.extend(dates_urls_titles_on_page_filial(next_page)[2])\n",
    "\n",
    "# Количество новых статей с даты последнего запуска агрегатора\n",
    "news_sum = sum([True for date_news in dates if str_to_date(date_news) > start_date])\n",
    "\n",
    "print(f'С даты последнего запуска агрегатора [{start_date}] опубликовано {news_sum} статей.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69976758",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "ext_urls = []\n",
    "\n",
    "for page_url in urls[:news_sum]:\n",
    "    print(page_url)\n",
    "    response = get_html(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    content.append(get_content(page_url)[0])\n",
    "    ext_urls.append(get_content(page_url)[1])\n",
    "    clear_output(wait=True)\n",
    "    print(len(dates[:news_sum]),len(urls[:news_sum]),len(titles[:news_sum]),len(content),len(ext_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086dcb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame()\n",
    "df3['Дата'] = dates[:news_sum]\n",
    "df3['Раздел'] = '******'\n",
    "df3['Заголовок'] = titles[:news_sum]\n",
    "df3['Текст'] = content\n",
    "df3['Внешние ссылки'] = ext_urls\n",
    "df3['URL статьи'] = urls[:news_sum]\n",
    "\n",
    "# корректируем столбцы 'Текст' и 'Внешние ссылки'\n",
    "for column in list(df3)[3:5]:\n",
    "    df3 = flatten_column(df3,column)\n",
    "df3.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538bbd89",
   "metadata": {},
   "source": [
    "###### Раздел \"Город4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da522591",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://****.h*******.**/news/page'\n",
    "start_page = base_url + str(1) + '.html'\n",
    "print(f'start_page = {start_page}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddfce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "urls = []\n",
    "titles = []\n",
    "dates.extend(dates_urls_titles_on_page_filial(start_page)[0])\n",
    "urls.extend(dates_urls_titles_on_page_filial(start_page)[1])\n",
    "titles.extend(dates_urls_titles_on_page_filial(start_page)[2])\n",
    "\n",
    "count=1\n",
    "while str_to_date(dates[-1]) > start_date:\n",
    "    count += 1\n",
    "    next_page = base_url + str(count) + '.html'\n",
    "    dates.extend(dates_urls_titles_on_page_filial(next_page)[0])\n",
    "    urls.extend(dates_urls_titles_on_page_filial(next_page)[1])\n",
    "    titles.extend(dates_urls_titles_on_page_filial(next_page)[2])\n",
    "\n",
    "# Количество новых статей с даты последнего запуска агрегатора\n",
    "news_sum = sum([True for date_news in dates if str_to_date(date_news) > start_date])\n",
    "\n",
    "print(f'С даты последнего запуска агрегатора [{start_date}] опубликовано {news_sum} статей.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "ext_urls = []\n",
    "\n",
    "for page_url in urls[:news_sum]:\n",
    "    print(page_url)\n",
    "    response = get_html(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    content.append(get_content(page_url)[0])\n",
    "    ext_urls.append(get_content(page_url)[1])\n",
    "    clear_output(wait=True)\n",
    "    print(len(dates[:news_sum]),len(urls[:news_sum]),len(titles[:news_sum]),len(content),len(ext_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b894db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame()\n",
    "df4['Дата'] = dates[:news_sum]\n",
    "df4['Раздел'] = '*****'\n",
    "df4['Заголовок'] = titles[:news_sum]\n",
    "df4['Текст'] = content\n",
    "df4['Внешние ссылки'] = ext_urls\n",
    "df4['URL статьи'] = urls[:news_sum]\n",
    "\n",
    "# корректируем столбцы 'Текст' и 'Внешние ссылки'\n",
    "for column in list(df4)[3:5]:\n",
    "    df4 = flatten_column(df4,column)\n",
    "df4.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = base_url.split('/')[2].split('.')[1] + '_' + str(datetime.now().date()) + '.xlsx'\n",
    "with pd.ExcelWriter(file_name) as writer:\n",
    "    df.to_excel(writer, sheet_name='*****', index=False)\n",
    "    df2.to_excel(writer, sheet_name='******', index=False)\n",
    "    df3.to_excel(writer, sheet_name='*******', index=False)\n",
    "    df4.to_excel(writer, sheet_name='******', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a63d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#обновление хранимого id-объявлений и даты в файле aggregator.ini\n",
    "write_conf(FQDN, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5137ba",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
